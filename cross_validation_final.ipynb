{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import math\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de variáveis de aplicação geral\n",
    "divisao_cjt_teste = 0.2\n",
    "caminho_graficos = r'C:\\Users\\alexa\\OneDrive\\Documentos\\TCC MBA\\Etapa final TCC\\Ambiente de testes\\Testes consolidados\\Gráficos Kfold'\n",
    "caminho_base = r'C:\\Users\\alexa\\OneDrive\\Documentos\\TCC MBA\\Etapa final TCC\\Ambiente de testes\\Dados.xlsx'\n",
    "caminho_pasta_atual = r'C:\\Users\\alexa\\OneDrive\\Documentos\\TCC MBA\\Etapa final TCC\\Ambiente de testes\\Testes consolidados'\n",
    "caminho_modelos = r'C:\\Users\\alexa\\OneDrive\\Documentos\\TCC MBA\\Etapa final TCC\\Ambiente de testes\\Testes consolidados\\Métricas.xlsx'\n",
    "planilha_modelos = 'Modelos kfold'\n",
    "planilha_original = 'DadosOriginais'\n",
    "planilha_oversample = 'DadosOS'\n",
    "planilha_smote = 'DadosSMOTE'\n",
    "output = 'output-kfold.txt'\n",
    "# Definição das seeds\n",
    "seed_value = 1\n",
    "random.seed(seed_value)         # Python random\n",
    "np.random.seed(seed_value)      # Numpy\n",
    "tf.random.set_seed(seed_value)  # TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação da base de dados\n",
    "modelos = pd.read_excel(io=caminho_modelos, sheet_name=planilha_modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição da função para tratamento dos dados. Retorna os dados já divididos em treinamento e teste\n",
    "def tratamento_dados(caminho_arquivo:str, nome_planilha:str, aplica_undersample:bool):\n",
    "    \n",
    "    # Importação dos dados para um DataFrame pandas\n",
    "    dados = pd.read_excel(io=caminho_arquivo, sheet_name=nome_planilha)\n",
    "\n",
    "    # Excluindo o campo Data, visto que o ano em si não é importante na análise, apenas a janela temporal\n",
    "    try:\n",
    "        dados.drop(['Data'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    dados_tratamento = None\n",
    "\n",
    "    if aplica_undersample:\n",
    "\n",
    "        # Aplicando a técnica de undersampling para diminuir a quantidade da categoria predominante (RJ = 0) e balancear os dados\n",
    "        # Filtrando os dados e pegando apenas 1 registro de cada empresa, do ano 5, que contém de fato 0 e 1\n",
    "        dados_undersampling = dados[dados['Ano'] == 5][['Name', 'RJ']]\n",
    "\n",
    "        #Transformando em um array de 2 dimensões para ser utilizado com a classe RandomUnderSampler, e separando em x e y\n",
    "        x_undersampling = dados_undersampling['Name'].values.reshape(-1, 1)\n",
    "        y_undersampling = dados_undersampling['RJ']\n",
    "\n",
    "        # Construindo o objeto para aplicar a reamostragem, com fator de 30% da classe menor em relação à maior\n",
    "        rus = RandomUnderSampler(sampling_strategy=0.3, random_state=42)\n",
    "\n",
    "        # Aplicando a reamostragem\n",
    "        x_resampled, _ = rus.fit_resample(x_undersampling, y_undersampling)\n",
    "\n",
    "        # Transformando o array x, que contém o nome das empresas mantidas após RUS em um array de uma dimensão\n",
    "        empresas_remanescentes = x_resampled.flatten()\n",
    "\n",
    "        # Filtrando os dados originais e mantendo apenas os nomes das empresas que saíram do resultado de RUS\n",
    "        dados_tratamento = dados[dados['Name'].isin(empresas_remanescentes)]\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Caso RUS não seja ativado na chamada da função, simplesmente ignora o processo\n",
    "        dados_tratamento = dados\n",
    " \n",
    "\n",
    "    # Inicializando uma variável de lista vazia para conter os dados organizados para a análise\n",
    "    dados_organizados = []\n",
    "\n",
    "    # Criando uma variável com os nomes (códigos) distintos de todas empresas\n",
    "    empresas_unicas = dados_tratamento['Name'].unique()\n",
    "\n",
    "    # Iterando sobre os nomes distintos das empresas e separando os dados dos indicadores com o nome de cada empresa em uma lista\n",
    "    for empresa in empresas_unicas:\n",
    "        dados_empresa = dados_tratamento[dados_tratamento['Name'] == empresa]\n",
    "        dados_empresa = dados_empresa.iloc[:,0:-2].values\n",
    "        dados_organizados.append(dados_empresa)\n",
    "    \n",
    "    # Transformando a lista em um array numpy\n",
    "    dados_organizados = np.array(dados_organizados)\n",
    "\n",
    "    # Atribuindo o array à variável X, que representa as variáveis independentes do modelo\n",
    "    X = dados_organizados\n",
    "\n",
    "    # Atribuindo à variável y, que represente a variável dependente, os valores da variável dependente RJ\n",
    "    y = dados_tratamento[dados_tratamento['Ano'] == 5]['RJ']\n",
    "\n",
    "    return train_test_split(X, y, test_size=divisao_cjt_teste, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função que constrói o modelo de rede neural, retornando-a ao fim\n",
    "def return_model(input_shape, lstm:int, lstm2:str, dense2:str, dropout_rate:float, learning_rate:float, ativacao:str):\n",
    "\n",
    "    # Define um modelo sequencial LSTM\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adiciona uma camada lstm\n",
    "    model.add(LSTM(lstm, input_shape=input_shape, activation=ativacao, return_sequences=True if lstm2 == 's' else False))\n",
    "\n",
    "    # Adiciona dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Camadas adicionais condicionais\n",
    "    if lstm2 == 's':\n",
    "        model.add(LSTM(lstm, activation=ativacao, return_sequences=False))\n",
    "    if dense2 == 's':\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(32, activation=ativacao))\n",
    "\n",
    "    # Adiciona a camada densa de saída\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Define o otimizador\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compila o modelo\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[BinaryAccuracy()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uma função para normalização dos dados\n",
    "def normalizacao(caminho_base_, planilha_, undersample_):\n",
    "\n",
    "    # Atriubui valores às variáveis independentes (x) e dependentes (y) de treinamento e teste, utilizando função previamente definida\n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(caminho_base_, planilha_, undersample_)\n",
    "\n",
    "    # Separa as quantidades de empresas para teste e validação, utilizada a seguir para remodelar uma matriz\n",
    "    qtd_empresas_teste = y_test.shape[0]\n",
    "    qtd_empresas_treino = y_train.shape[0]\n",
    "\n",
    "    # Separando o nome das empresas de treino em uma variável exclusiva, para posterior identificação caso necessário\n",
    "    empresas_treino = X_train[:,:,0:1]\n",
    "\n",
    "    # Remodela empresas treino para um array de uma dimensão\n",
    "    empresas_treino = empresas_treino.reshape(qtd_empresas_treino, 5)\n",
    "\n",
    "    empresas_treino = empresas_treino[:,0]\n",
    "\n",
    "    # Separando os dados quantitativos (indicadores) e redefinindo a variável X_train com os mesmos\n",
    "    X_train = X_train[:,:,1:5]\n",
    "\n",
    "    # Definindo a variável n_features com base no número de indicadores atual do estudo\n",
    "    _,_,n_features = X_train.shape\n",
    "\n",
    "    # Realizando os mesmos procedimentos nas empresas de teste\n",
    "    empresas_teste = X_test[:,:,0:1]\n",
    "    empresas_teste = empresas_teste.reshape(qtd_empresas_teste, 5)\n",
    "    empresas_teste = empresas_teste[:,0]\n",
    "    X_test = X_test[:,:,1:5]\n",
    "\n",
    "    # Inicializando uma lista vazia onde serão armazenados os scalers, os objetos que farão a normalização dos dados de teste e treino\n",
    "    scalers = []\n",
    "\n",
    "    # Inicializa arrays para guardar os dados normalizados\n",
    "    X_train_normalized = np.empty(X_train.shape)\n",
    "    X_test_normalized = np.empty(X_test.shape)\n",
    "\n",
    "\n",
    "    # Iteração partindo de zero até n_features vezes\n",
    "    for i in range(n_features):\n",
    "\n",
    "        # Inicializa o objeto scaler a ser utilizado nesta iteração/indicador\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        '''\n",
    "            Aplica o método fit_transform sobre o i-ésimo indicador - este método calcula média e desvpad,\n",
    "            subtraindo a média dos valores e dividindo esse resultado pelo desvpad para normalizá-los.\n",
    "        '''\n",
    "         \n",
    "        X_train_normalized[:, :, i] = scaler.fit_transform(X_train[:, :, i])\n",
    "\n",
    "        # O método transform dessa vez é aplicado sobre a base de teste, utilizando o mesmo desvpad e média da base de treino\n",
    "        X_test_normalized[:, :, i] = scaler.transform(X_test[:, :, i])\n",
    "        \n",
    "        # Armazena o objeto scaler para posteridade, caso se deseje desnormalizar os dados\n",
    "        scalers.append(scaler)\n",
    "    return X_train_normalized, X_test_normalized, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função que realizará a validação propriamente dita\n",
    "def kfold_custom(pesos, lstm, lstm2, dense2, tx_dropout, tx_aprendizagem, tamanho_lote, epocas, ativacao, caminho_base_, planilha_, undersample_, n_splits_, alias, trial, ajuste_epocas):\n",
    "\n",
    "        # Inicializa variáveis de lista para receberem os resultados das métricas de avaliação\n",
    "        results_acc = []\n",
    "        results_prec = []\n",
    "        results_rec = []\n",
    "        results_f1 = []\n",
    "        results_auc = []\n",
    "\n",
    "        # Aplica normalização nos dados da base        \n",
    "        X_train_normalized, X_test_normalized, y_train, y_test = normalizacao(caminho_base_, planilha_, undersample_)\n",
    "\n",
    "        # Extrai a janela temporal e número de indicadores do formato da matriz/array X_train_normalized        \n",
    "        _, anos, indicadores = X_train_normalized.shape\n",
    "\n",
    "        # Define a variável que será utilizada no formato de entrada dos dados na arquitetura do modelo\n",
    "        input_shape = (anos, indicadores)\n",
    "\n",
    "        # Inicializa um objeto da classe KFold para realizar as divisões dos dados em cada dobra        \n",
    "        kfold = KFold(n_splits=n_splits_, shuffle=True, random_state=42)\n",
    "\n",
    "        # Transforma y_train em array\n",
    "        y_train_array = y_train.values\n",
    "\n",
    "        # Variável que determina quantas linhas de gráficos haverão na figura gerada para cada modelo\n",
    "        linhas = math.ceil(n_splits_ / 2)\n",
    "\n",
    "        # Constrói o objeto onde os gráficos serão plotados\n",
    "        fig, axs = plt.subplots(linhas, 2, figsize=(15, 5 * linhas))\n",
    "        fig.suptitle('Perda de Treino vs. Validação por Dobra do KFold')\n",
    "\n",
    "        # Inicia iteração sobre cada dobra gerada pelo objeto kfold com base nos dados de treino normalizados\n",
    "        fold_num = 1\n",
    "        for i, (train_index, test_index) in enumerate(kfold.split(X_train_normalized)):\n",
    "                \n",
    "                # Dividindo os dados de treino e teste da dobra, com base na parcela de dados da divisão de treino geral\n",
    "                X_train_fold = X_train_normalized[train_index]\n",
    "                X_test_fold = X_train_normalized[test_index]\n",
    "                y_train_fold = y_train_array[train_index]\n",
    "                y_test_fold = y_train_array[test_index]\n",
    "\n",
    "                # Retorna o modelo que será utilizado\n",
    "                model = return_model(input_shape, lstm, lstm2, dense2, tx_dropout, tx_aprendizagem, ativacao)\n",
    "\n",
    "                # Se o argumento pesos for verdadeiro, cria o objeto class_weight_dict\n",
    "                if pesos:\n",
    "\n",
    "                        classes = np.unique(y_train)\n",
    "                        class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "                        class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "                '''\n",
    "                Verifica se a variável ajuste_epocas já é verdadeira. \n",
    "                Isto é necessária para que todas dobras de um modelo sejam treinadas com o mesmo número de épocas, \n",
    "                devido ao early stopping que pode ser aplicado de forma diferente a cada iteração\n",
    "                '''\n",
    "                if not ajuste_epocas:\n",
    "\n",
    "                        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                                patience=5,\n",
    "                                                restore_best_weights=True)\n",
    "                        \n",
    "                        # Executa o treinamento do modelo\n",
    "                        history = model.fit(\n",
    "                                X_train_fold, y_train_fold, epochs=epocas, batch_size=tamanho_lote, \n",
    "                                validation_split=divisao_cjt_teste, verbose=0, callbacks=[early_stopping],\n",
    "                                class_weight=class_weight_dict if pesos else None\n",
    "                                )\n",
    "                else:\n",
    "                        # Executa o treinamento do modelo\n",
    "                        history = model.fit(\n",
    "                                X_train_fold, y_train_fold, epochs=epocas, batch_size=tamanho_lote, \n",
    "                                validation_split=divisao_cjt_teste, verbose=0,\n",
    "                                class_weight=class_weight_dict if pesos else None\n",
    "                                )    \n",
    "\n",
    "                # Separa o histórico da curva de perda no treino e validação     \n",
    "                train_losses = history.history['loss']\n",
    "                val_losses = history.history['val_loss']\n",
    "\n",
    "                '''\n",
    "                Realiza as previsões no conjunto de teste da dobra, transformando o resultado para binário.\n",
    "                Necessário para que tanto o Y predito quanto o real correspondam ao mesmo tipo de dado\n",
    "                '''\n",
    "                y_pred = model.predict(X_test_fold).flatten()\n",
    "                y_pred_binario = (y_pred > 0.5).astype(int)\n",
    "\n",
    "                # Ajusta o numero de épocas na primeira iteração, conforme early stopping\n",
    "                if not ajuste_epocas:\n",
    "                       epocas = len(history.history['loss'])\n",
    "                       ajuste_epocas = True\n",
    "\n",
    "                # Cálculo das métricas\n",
    "                acc = round(accuracy_score(y_test_fold, y_pred_binario), 4)\n",
    "                prec = round(precision_score(y_test_fold, y_pred_binario), 4)\n",
    "                rec = round(recall_score(y_test_fold, y_pred_binario), 4)\n",
    "                f1 = round(f1_score(y_test_fold, y_pred_binario), 4)\n",
    "                auc_roc = round(roc_auc_score(y_test_fold, y_pred_binario), 4)\n",
    "\n",
    "                # Atualização da variável de lista conforme métricas da dobra\n",
    "                results_acc.append(acc)\n",
    "                results_prec.append(prec)\n",
    "                results_rec.append(rec)\n",
    "                results_f1.append(f1)\n",
    "                results_auc.append(auc_roc)\n",
    "\n",
    "                # Print no console para acompanhamento\n",
    "                print(f\"Dobra {fold_num}\")\n",
    "                print(\"Accuracy:\", acc)\n",
    "                print(\"Precision:\", prec)\n",
    "                print(\"Recall:\", rec)\n",
    "                print(\"F1 Score:\", f1)\n",
    "                print(\"ROC AUC:\", auc_roc)\n",
    "\n",
    "                # Calcula qual a próxima posição (linha, coluna) será plotado gráfico na figura criada anteriormente\n",
    "                linha = 0 if i < 1 else linha + 1 if i % 2 == 0 else linha\n",
    "                coluna = 1 if i % 2 > 0 else 0\n",
    "\n",
    "                # Criando e plotando o gráfico da dobra atual no conjunto de gráficos do modelo\n",
    "                ax = axs[linha, coluna]\n",
    "                ax.plot(train_losses, label='Perda de Treino')\n",
    "                ax.plot(val_losses, label='Perda de Validação')\n",
    "                ax.set_title(f'Dobra {fold_num}')\n",
    "                ax.set_xlabel('Épocas')\n",
    "                ax.set_ylabel('Perda')\n",
    "                ax.legend()\n",
    "                \n",
    "                # Variável de controle das dobras\n",
    "                fold_num += 1\n",
    "                \n",
    "        # Configura e salva os gráficos do modelo atual\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.ylim(bottom=0, top=1)\n",
    "        plt.savefig(os.path.join(caminho_graficos, f'modelo_{trial}_{alias}.png'))\n",
    "        plt.close()\n",
    "        metricas = f'Alias: {alias}, Trial: {trial}, Acc: {results_acc}, Prec: {results_prec}, Rec: {results_rec}, F1: {results_f1}, AUC_ROC: {results_auc}, Épocas ajustadas: {epocas}'\n",
    "\n",
    "        # Salva os dados das métricas no arquivo de texto\n",
    "        with open(os.path.join(caminho_pasta_atual, output), 'a') as f:\n",
    "            print(metricas, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Itera sobre as linhas da tabela Pandas contendo os dados com os parâmetros dos modelos selecionados para a validação cruzada,\n",
    "    construindo-os e inicializando todo o processo\n",
    "'''\n",
    "for index, row in modelos.iterrows():\n",
    "    alias = row['Alias']\n",
    "    trial = row['Trial']\n",
    "    lstm = row['lstm']\n",
    "    lstm2 = row['lstm2']\n",
    "    dense2 = row['dense2']\n",
    "    ativacao = row['ativacao']\n",
    "    dropout_rate = row['dropout']\n",
    "    learning_rate = row['learning_rate']\n",
    "    batch_size = row['batch_size']\n",
    "    epochs = row['epochs']\n",
    "\n",
    "    print(f'Calculando {alias}-{trial}, índice {index} de {modelos.shape[0]}')\n",
    "\n",
    "    # Chama a função principal, passando variáveis conforme cada estratégia definida\n",
    "    kfold_custom(\n",
    "        pesos=True if 'P' in alias else False, \n",
    "        lstm=lstm, \n",
    "        lstm2=0 if lstm2 == 'n' else lstm, \n",
    "        dense2=0 if dense2 == 'n' else 32, \n",
    "        tx_dropout=dropout_rate, \n",
    "        tx_aprendizagem=learning_rate, \n",
    "        tamanho_lote=batch_size, \n",
    "        epocas=epochs, \n",
    "        ativacao=ativacao, \n",
    "        caminho_base_=caminho_base, \n",
    "        planilha_=planilha_oversample if 'OS' in alias else planilha_smote if 'SMOTE' in alias else planilha_original, \n",
    "        undersample_=True if 'US' in alias else False,\n",
    "        n_splits_ = 5,\n",
    "        alias = alias,\n",
    "        trial = trial,\n",
    "        ajuste_epocas = False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
