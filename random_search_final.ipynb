{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importação das bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import optuna\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de algumas variáveis globais\n",
    "divisao_cjt_teste = 0.2\n",
    "caminho_graficos = r'C:\\Users\\alexa\\OneDrive\\Documentos\\TCC MBA\\Etapa final TCC\\Ambiente de testes\\Gráficos overfitting'\n",
    "caminho_base = r'C:\\Users\\alexa\\OneDrive\\Documentos\\TCC MBA\\Etapa final TCC\\Ambiente de testes\\Dados.xlsx'\n",
    "caminho_pasta_atual = r'C:\\Users\\alexa\\OneDrive\\Documentos\\TCC MBA\\Etapa final TCC\\Ambiente de testes'\n",
    "planilha_original = 'DadosOriginais'\n",
    "planilha_oversample = 'DadosOS'\n",
    "planilha_smote = 'DadosSmote'\n",
    "\n",
    "# Definindo valor da seed das principais bibliotecas para replicação dos resultados\n",
    "seed_value = 1\n",
    "random.seed(seed_value)         # Python random\n",
    "np.random.seed(seed_value)      # Numpy\n",
    "tf.random.set_seed(seed_value)  # TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição da função para tratamento dos dados. Retorna os dados já divididos em treinamento e teste\n",
    "def tratamento_dados(caminho_arquivo:str, nome_planilha:str, aplica_undersample:bool):\n",
    "    \n",
    "    # Importação dos dados para um DataFrame pandas\n",
    "    dados = pd.read_excel(io=caminho_arquivo, sheet_name=nome_planilha)\n",
    "\n",
    "    # Declarando a variável que receberá os valores dos dados que serão tratados\n",
    "    dados_tratamento = None\n",
    "\n",
    "    # Inicializando a variável dados_tratamento com um valor diferente de nulo conforme argumento passado na assinatura da função\n",
    "    if aplica_undersample:\n",
    "\n",
    "        # Aplicando a técnica de undersampling para diminuir a quantidade da categoria predominante (RJ = 0) e balancear os dados\n",
    "        # Filtrando os dados e pegando apenas 1 registro de cada empresa, do ano 5, que contém de fato 0 e 1\n",
    "        dados_undersampling = dados[dados['Ano'] == 5][['Name', 'RJ']]\n",
    "\n",
    "        # Transformando em um array de 2 dimensões para ser utilizado com a classe RandomUnderSampler, e separando em x e y\n",
    "        x_undersampling = dados_undersampling['Name'].values.reshape(-1, 1)\n",
    "        y_undersampling = dados_undersampling['RJ']\n",
    "\n",
    "        # Construindo o objeto para aplicar a reamostragem, com fator de 30% da classe menor em relação à maior\n",
    "        rus = RandomUnderSampler(sampling_strategy=0.3, random_state=42)\n",
    "\n",
    "        # Aplicando a reamostragem\n",
    "        x_resampled, _ = rus.fit_resample(x_undersampling, y_undersampling)\n",
    "\n",
    "        # Transformando o array x, que contém o nome das empresas mantidas após RUS em um array de uma dimensão\n",
    "        empresas_remanescentes = x_resampled.flatten()\n",
    "\n",
    "        # Filtrando os dados originais e mantendo apenas os nomes das empresas que saíram do resultado de RUS\n",
    "        dados_tratamento = dados[dados['Name'].isin(empresas_remanescentes)]\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Caso RUS não seja ativado na chamada da função, simplesmente ignora o processo\n",
    "        dados_tratamento = dados\n",
    " \n",
    "\n",
    "    # Inicializando uma variável de lista vazia para conter os dados organizados para a análise\n",
    "    dados_organizados = []\n",
    "\n",
    "    # Criando uma variável com os nomes (códigos) distintos de todas empresas\n",
    "    empresas_unicas = dados_tratamento['Name'].unique()\n",
    "\n",
    "    # Iterando sobre os nomes distintos das empresas e separando os dados dos indicadores com o nome de cada empresa em uma lista\n",
    "    for empresa in empresas_unicas:\n",
    "        dados_empresa = dados_tratamento[dados_tratamento['Name'] == empresa]\n",
    "        dados_empresa = dados_empresa.iloc[:,0:-2].values\n",
    "        dados_organizados.append(dados_empresa)\n",
    "    \n",
    "    # Transformando a lista em um array numpy\n",
    "    dados_organizados = np.array(dados_organizados)\n",
    "\n",
    "    # Atribuindo o array à variável X, que representa as variáveis independentes do modelo\n",
    "    X = dados_organizados\n",
    "\n",
    "    # Atribuindo à variável y, que represente a variável dependente, os valores da variável dependente RJ\n",
    "    y = dados_tratamento[dados_tratamento['Ano'] == 5]['RJ']\n",
    "\n",
    "    return train_test_split(X, y, test_size=divisao_cjt_teste, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função que constrói o modelo de rede neural, retornando-a ao fim\n",
    "def return_model(input_shape, lstm:int, lstm2:str, dense2:str, dropout_rate:float, learning_rate:float, ativacao:str):\n",
    "\n",
    "    # Define um modelo sequencial LSTM\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adiciona uma camada lstm\n",
    "    model.add(LSTM(lstm, input_shape=input_shape, activation=ativacao, return_sequences=True if lstm2 == 's' else False))\n",
    "\n",
    "    # Adiciona dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Camadas adicionais condicionais\n",
    "    if lstm2 == 's':\n",
    "        model.add(LSTM(lstm, activation=ativacao, return_sequences=False))\n",
    "    if dense2 == 's':\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(32, activation=ativacao))\n",
    "\n",
    "    # Adiciona a camada densa de saída\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Define o otimizador\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compila o modelo\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[BinaryAccuracy()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função que será chamada a cada estratégia\n",
    "def corrida(alias:str):\n",
    "\n",
    "    # Define o conjunto de variáveis que rege cada cenário para cada estratégia\n",
    "    if alias == 'IN':\n",
    "        pesos = False\n",
    "        planilha = planilha_original\n",
    "        undersample = False\n",
    "    elif alias == 'P':\n",
    "        pesos = True\n",
    "        planilha = planilha_original\n",
    "        undersample = False\n",
    "    elif alias == 'OS+P':\n",
    "        pesos = True\n",
    "        planilha = planilha_oversample\n",
    "        undersample = False\n",
    "    elif alias == 'OS':\n",
    "        pesos = False\n",
    "        planilha = planilha_oversample\n",
    "        undersample = False\n",
    "    elif alias == 'US':\n",
    "        pesos = False\n",
    "        planilha = planilha_original\n",
    "        undersample = True\n",
    "    elif alias == 'US+P':\n",
    "        pesos = True\n",
    "        planilha = planilha_original\n",
    "        undersample = True\n",
    "    elif alias == 'US+OS+P':\n",
    "        pesos = True\n",
    "        planilha = planilha_oversample\n",
    "        undersample = True\n",
    "    elif alias == 'SMOTE':\n",
    "        pesos = False\n",
    "        planilha = planilha_smote\n",
    "        undersample = False\n",
    "    else:\n",
    "        return Exception('Erro ao escolher o alias da corrida')\n",
    "    \n",
    "    # Atriubui valores às variáveis independentes (x) e dependentes (y) de treinamento e teste, utilizando função previamente definida\n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(caminho_base, planilha, undersample)\n",
    "\n",
    "    # Separa as quantidades de empresas para teste e validação, utilizada a seguir para remodelar uma matriz\n",
    "    qtd_empresas_teste = y_test.shape[0]\n",
    "    qtd_empresas_treino = y_train.shape[0]\n",
    "\n",
    "    # Separando o nome das empresas de treino em uma variável exclusiva, para posterior identificação caso necessário\n",
    "    empresas_treino = X_train[:,:,0:1]\n",
    "\n",
    "    # Remodela empresas treino para um array de uma dimensão\n",
    "    empresas_treino = empresas_treino.reshape(qtd_empresas_treino, 5)\n",
    "\n",
    "    empresas_treino = empresas_treino[:,0]\n",
    "\n",
    "    # Separando os dados quantitativos (indicadores) e redefinindo a variável X_train com os mesmos\n",
    "    X_train = X_train[:,:,1:5]\n",
    "\n",
    "    # Definindo a variável n_features com base no número de indicadores atual do estudo\n",
    "    _,_,n_features = X_train.shape\n",
    "\n",
    "    # Realizando os mesmos procedimentos nas empresas de teste\n",
    "    empresas_teste = X_test[:,:,0:1]\n",
    "    empresas_teste = empresas_teste.reshape(qtd_empresas_teste, 5)\n",
    "    empresas_teste = empresas_teste[:,0]\n",
    "    X_test = X_test[:,:,1:5]\n",
    "\n",
    "    # Inicializando uma lista vazia onde serão armazenados os scalers, os objetos que farão a normalização dos dados de teste e treino\n",
    "    scalers = []\n",
    "\n",
    "    # Inicializa arrays para guardar os dados normalizados\n",
    "    X_train_normalized = np.empty(X_train.shape)\n",
    "    X_test_normalized = np.empty(X_test.shape)\n",
    "\n",
    "\n",
    "    # Iteração partindo de zero até n_features vezes\n",
    "    for i in range(n_features):\n",
    "\n",
    "        # Inicializa o objeto scaler a ser utilizado nesta iteração/indicador\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        '''\n",
    "            Aplica o método fit_transform sobre o i-ésimo indicador - este método calcula média e desvpad,\n",
    "            subtraindo a média dos valores e dividindo esse resultado pelo desvpad para normalizá-los.\n",
    "        '''\n",
    "         \n",
    "        X_train_normalized[:, :, i] = scaler.fit_transform(X_train[:, :, i])\n",
    "\n",
    "        # O método transform dessa vez é aplicado sobre a base de teste, utilizando o mesmo desvpad e média da base de treino\n",
    "        X_test_normalized[:, :, i] = scaler.transform(X_test[:, :, i])\n",
    "        \n",
    "        # Armazena o objeto scaler para posteridade, caso se deseje desnormalizar os dados\n",
    "        scalers.append(scaler)\n",
    "\n",
    "    # Define a função objetiva que será utilizada pela Optuna na Random Search\n",
    "    def objetivo(trial):\n",
    "\n",
    "        # Define os limites com os quais o Optuna vai trabalhar para cada variável\n",
    "        lstm = trial.suggest_int('lstm', 10, 100, step=10)\n",
    "        lstm2 = trial.suggest_categorical('lstm2', ['s', 'n'])\n",
    "        dense2 = trial.suggest_categorical('dense2', ['s', 'n'])\n",
    "        ativacao = trial.suggest_categorical('ativacao', ['relu', 'tanh'])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.2)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01)\n",
    "\n",
    "        # Extrai a janela temporal e número de indicadores do formato da matriz/array X_train_normalized\n",
    "        _, anos, indicadores = X_train_normalized.shape\n",
    "\n",
    "        # Define a variável que será utilizada no formato de entrada dos dados na arquitetura do modelo\n",
    "        input_shape = (anos, indicadores)\n",
    "\n",
    "        # Retorna o modelo que será utilizado por iteração de cada estratégia\n",
    "        model = return_model(input_shape, lstm, lstm2, dense2, dropout_rate, learning_rate, ativacao)\n",
    "\n",
    "        # Mais definição de limites para a Optuna\n",
    "        batch_size = int(trial.suggest_categorical('batch_size', ['32', '64', '128', '192']))\n",
    "        \n",
    "        epochs = trial.suggest_int('epochs', 10, 310, step=20)\n",
    "\n",
    "        # Se o argumento pesos for verdadeiro, cria o objeto class_weight_dict\n",
    "        if pesos:\n",
    "\n",
    "            classes = np.unique(y_train)\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "            class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "        # Define o early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                patience=5,\n",
    "                                restore_best_weights=True)\n",
    "\n",
    "        # Treina o modelo\n",
    "        history = model.fit(X_train_normalized, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                    validation_split=divisao_cjt_teste, verbose=0, callbacks=[early_stopping],\n",
    "                    class_weight=class_weight_dict if pesos else None)  \n",
    "\n",
    "        # Separa o histórico da curva de perda no treino e validação    \n",
    "        train_losses = history.history['loss']\n",
    "        val_losses = history.history['val_loss']\n",
    "\n",
    "        '''\n",
    "        Realiza as previsões no conjunto de teste, transformando o resultado para binário.\n",
    "        Necessário para que tanto o Y predito quanto o real correspondam ao mesmo tipo de dado\n",
    "        '''\n",
    "        y_pred = model.predict(X_test_normalized)\n",
    "        y_pred_binario = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        # Cálculo das métricas\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_binario)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred_binario), 4)\n",
    "        precision = round(precision_score(y_test, y_pred_binario), 4)\n",
    "        recall = round(recall_score(y_test, y_pred_binario), 4)\n",
    "        f1 = round(f1_score(y_test, y_pred_binario), 4)\n",
    "        roc_auc = round(roc_auc_score(y_test, y_pred_binario), 4)\n",
    "\n",
    "        # Construção do gráfico\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Perda de Treino')\n",
    "        plt.plot(val_losses, label='Perda de Validação')\n",
    "        plt.title('Perda de Treino vs. Validação')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Perda')\n",
    "        plt.ylim(bottom=0, top=1)\n",
    "        plt.legend()\n",
    "\n",
    "        x0, xmax = plt.xlim()\n",
    "        y0, ymax = plt.ylim()\n",
    "\n",
    "        data_width = xmax - x0\n",
    "        data_height = ymax - y0\n",
    "        \n",
    "        plt.text(data_width * 0.1, data_height * 0.1, f\"Acc {accuracy} | Prec {precision} | Recall {recall} | F1 {f1} | ROC AUC {roc_auc}\", \n",
    "                fontsize=8, color='red')\n",
    "        plt.savefig(os.path.join(caminho_graficos, f'modelo_{trial.number}_{alias}.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Salva o output em txt\n",
    "        with open(os.path.join(caminho_pasta_atual, 'output.txt'), 'a') as f:\n",
    "            print(f'Alias: {alias}, Trial: {trial.number}, acc: {accuracy}, prec: {precision}, recall: {recall}, f1: {f1}, roc_auc: {roc_auc}, matriz_confusao: {conf_matrix}, params: {trial.params}', file=f)\n",
    "        return accuracy * recall * roc_auc\n",
    "    \n",
    "    # Cria um objeto \"estudo\" na Optuna e inicializa a otimização passando a função objetivo e o número de iterações\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objetivo, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chama a função corrida para cada estratégia proposta\n",
    "for alias in ['IN', 'P', 'OS+P', 'OS', 'US', 'US+P', 'US+OS+P', 'SMOTE']:\n",
    "    corrida(alias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
